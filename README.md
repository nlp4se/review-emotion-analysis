# Emotion analysis from app reviews - Replication package

## üìö Summary

This repository contains the code and data for the replication package for the paper "What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile App Reviews".

## üìÇ Contents

- **Literature review**: results from the literature review on opinion mining and emotion analysis within the context of software-based reviews.
- **Data**: data used in the study, including user reviews (input), human annotations (ground truth), and LLM-based annotations (generated by the assistants).
- **Code**: code used in the study, including the generative annotation, data processing, and evaluation.

### üìñ Literature review

Study selection and results are available in the `literature_review/study-selection.xlsx` file. This file contains the following sheets:

- `iteration_1_IC_analysis`: results from the first iteration of the inclusion criteria analysis.
- `iteration_1_feature_extraction`: results from the first iteration of the feature extraction analysis.
- `iteration_2_IC_analysis`: results from the second iteration of the inclusion criteria analysis.
- `iteration_2_feature_extraction`: results from the second iteration of the feature extraction analysis.
- `iteration_3_IC_analysis`: results from the third iteration of the inclusion criteria analysis.
- `iteration_3_feature_extraction`: results from the third iteration of the feature extraction analysis.
- `emotions`: statistical analysis of emotions covered by emotion taxonomies in the selected studies.

### üóÉÔ∏è Data

The `data` root folder contains the following files:

- `reviews.json` contains the reviews used in the study.
- `guidelines.txt` contains a .txt version of the annotation guidelines.
- `ground-truth.xlsx` contains the ground truth (human agreement) annotations for the reviews.

In addition, the `data` root folder contains the following subfolders:
- `assistants` contains the IDs of the assistants used for the generative annotation (see [LLM-based annotation](#llm-based-annotation)).
- `annotations` contains the results of the human and LLM-based annotation:
-- `iterations` contains both human and LLM-based annotations for each iteration.
-- `llm-annotations` contains the LLM-based annotations for each assistance, including results for various temperature values: low (0), medium (0.5), and high (1) (see [LLM-based annotation](#llm-based-annotation)).
- `agreements` contains the results of the agreement analysis between the human and LLM-based annotations (see [Data Processing](#data-processing)).
- `evaluation` contains the results of the evaluation of the LLM-based annotations (see [Evaluation](#evaluation)), including statistics, Cohen's Kappa, correctness, and cost-efficiency analysis.

### üíª Code

We structure the code available in this replication package based on the stages involved in the LLM-based annotation process.

#### ü§ñ LLM-based annotation

The `llm_annotation` folder contains the code used to generate the LLM-based annotations.

There are two main scripts:

- `create_assistant.py` is used to create a new assistant with a particular provider and model. This class includes the definition of a common system prompt across all agents, using the `data/guidelines.txt` file as the basis.

- `annotate_emotions.py` is used to annotate a set of emotions using a previously created assistant. This script includes the assessment of the output format, as well as some common metrics for cost-efficiency analysis and output file generation.

Our research includes an LLM-based annotation experimentation with 3 LLMs: GPT-4o, Mistral Large 2, and Gemini 2.0 Flash. To illustrate the usage of the code, in this README we refer to the code execution for generating annotations using GPT-4o. However, full code is provided for all LLMs.

**üîë Step 1: Add your API key**

Add your API key to the `code/.env` file. For instance, for OpenAI, you can add the following:

```
OPENAI_API_KEY=sk-proj-...
```

**üõ†Ô∏è Step 2: Create an assistant**

Create an assistant using the `create_assistant.py` script. For instance, for GPT-4o, you can run the following command:

```python .\code\llm_annotation\create_assistant_openai.py --guidelines .\data\guidelines.txt --model gpt-4o```

This will create an assistant loading the `data/guidelines.txt` file and using the GPT-4o model.

**üìù Step 3: Annotate emotions**

Annotate emotions using the `annotate_emotions.py` script. For instance, for GPT-4o, you can run the following command:

```python .\code\llm_annotation\annotate_emotions_openai.py --input .\data\ground-truth.xlsx --output .\data\annotations\llm\temperature-00\ --batch_size 10 --model gpt-4o --temperature 0 --sleep_time 10```

Parameters include:

- `input`: path to the input file containing the set of reviews to annotate (e.g., `data/ground-truth.xlsx`).
- `output`: path to the output folder where annotations will be saved (e.g., `data/annotations/llm/temperature-00/`).
- `batch_size`: number of reviews to annotate for each user request (e.g., 10).
- `model`: model to use for the annotation (e.g., `gpt-4o`).
- `temperature`: temperature for the model responses (e.g., 0).
- `sleep_time`: time to wait between batches, in seconds (e.g., 10).

This will annotate the emotions using the assistant created in the previous step, creating a new file with the same format as in the `data/ground-truth.xlsx` file.

#### üîÑ Data processing

In this stage, we refactor all files into iterations and we consolidate the agreement between multiple annotators or LLM runs. These logic serves both for human and LLM annotations. Parameters can be updated to include more annotators or LLM runs.

**‚úÇÔ∏è Step 4: Split annotations into iterations**

We split the annotations into iterations based on the number of annotators or LLM runs. For instance, for GPT-4o (run 1), we can run the following command:

`python code\data_processing\split_annotations.py --input_file data\annotations\llm\temperature-00\gpt-4o-1-annotations.xlsx --output_dir data\annotations\iterations\`

This facilitates the Kappa analysis and agreement in alignment with each human iteration.

**ü§ù Step 5: Analyse agreement**

We consolidate the agreement between multiple annotators or LLM runs. For instance, for GPT-4o (run 1, 2, and 3), we can run the following command:

`python code\evaluation\agreement.py --input-folder data\annotations\iterations\ --output-folder data\agreements\ --annotators gpt-4o-1 gpt-4o-2 gpt-4o-3`

#### üìä Evaluation

After consolidating agreements, we can evaluate both the Cohen's Kappa agreement and correctness between the human and LLM-based annotations. Our code allows any combination of annotators and LLM runs.

**üìà Step 6: Emotion statistics**

We evaluate the statistics of the emotions in the annotations, including emotion frequency, distribution, and correlation between emotions. For instance, for GPT-4o, we can run the following command:

`python code\evaluation\emotion_statistics.py --input-file data\agreements\agreement_gpt-4o-1-gpt-4o-2-gpt-4o-3.xlsx --output-dir data\evaluation\statistics\gpt-4o`

**‚öñÔ∏è Step 7: Cohen's Kappa pairwise agreement**

We measure the average pairwise Cohen's Kappa agreement between annotators or LLM runs. For instance, for GPT-4o, we can run the following command:

`python code\evaluation\kappa.py --input_folder data\annotations\iterations\ --output_folder data\evaluation\kappa\ --annotators gpt-4o-1,gpt-4o-2,gpt-4o-3 --exclude 0,1,2`

In our analysis, we exclude iterations 0, 1 and 2 as they were used for guidelines refinement.

**‚úÖ Step 8: LLM-based annotation correctness**

We measure the correctness (accuracy, precision, recall, and F1 score) between a set of annotated reviews and a given ground truth. For instance, for GPT-4o agreement, we can run the following command:

`python code\evaluation\correctness.py --ground_truth data\ground-truth.xlsx --predictions data\agreements\agreement_gpt-4o-1-gpt-4o-2-gpt-4o-3.xlsx --output_dir data\evaluation\correctness\gpt-4o`

## üìú License

This repository is licensed under the GPL-3.0 License. See the LICENSE file for details.
